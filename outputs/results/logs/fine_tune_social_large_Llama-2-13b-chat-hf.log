nohup: ignoring input
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.90s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.
Dataset structure:
Train dataset columns: ['input_ids_text', 'labels_text']
Train dataset size: 588
Validation dataset size: 31
Renaming columns to match collator expectations...
Train dataset columns after renaming: ['source', 'target']
Training configuration:
  - Total training steps: 365
  - Evaluation every: 50 steps
  - Logging every: 10 steps
  0%|          | 0/370 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/370 [00:02<15:05,  2.45s/it]  1%|          | 2/370 [00:04<14:42,  2.40s/it]  1%|          | 3/370 [00:07<14:37,  2.39s/it]  1%|          | 4/370 [00:09<13:32,  2.22s/it]  1%|▏         | 5/370 [00:11<12:59,  2.14s/it]  2%|▏         | 6/370 [00:13<12:46,  2.11s/it]  2%|▏         | 7/370 [00:15<13:22,  2.21s/it]  2%|▏         | 8/370 [00:17<12:51,  2.13s/it]  2%|▏         | 9/370 [00:20<13:48,  2.30s/it]  3%|▎         | 10/370 [00:22<12:52,  2.15s/it]                                                  3%|▎         | 10/370 [00:22<12:52,  2.15s/it]  3%|▎         | 11/370 [00:24<13:51,  2.32s/it]  3%|▎         | 12/370 [00:26<13:09,  2.21s/it]  4%|▎         | 13/370 [00:28<12:53,  2.17s/it]  4%|▍         | 14/370 [00:31<12:57,  2.19s/it]  4%|▍         | 15/370 [00:33<12:59,  2.19s/it]  4%|▍         | 16/370 [00:35<12:53,  2.19s/it]  5%|▍         | 17/370 [00:37<12:16,  2.09s/it]  5%|▍         | 18/370 [00:39<12:28,  2.13s/it]  5%|▌         | 19/370 [00:41<12:29,  2.14s/it]  5%|▌         | 20/370 [00:43<12:42,  2.18s/it]                                                  5%|▌         | 20/370 [00:43<12:42,  2.18s/it]  6%|▌         | 21/370 [00:46<13:15,  2.28s/it]  6%|▌         | 22/370 [00:48<13:21,  2.30s/it]  6%|▌         | 23/370 [00:51<13:45,  2.38s/it]  6%|▋         | 24/370 [00:53<13:31,  2.35s/it]  7%|▋         | 25/370 [00:55<13:15,  2.30s/it]  7%|▋         | 26/370 [00:58<14:01,  2.45s/it]  7%|▋         | 27/370 [01:00<13:00,  2.27s/it]  8%|▊         | 28/370 [01:02<12:26,  2.18s/it]  8%|▊         | 29/370 [01:04<12:33,  2.21s/it]  8%|▊         | 30/370 [01:07<13:16,  2.34s/it]                                                  8%|▊         | 30/370 [01:07<13:16,  2.34s/it]  8%|▊         | 31/370 [01:09<12:21,  2.19s/it]  9%|▊         | 32/370 [01:11<12:39,  2.25s/it]  9%|▉         | 33/370 [01:13<12:07,  2.16s/it]  9%|▉         | 34/370 [01:15<11:40,  2.09s/it]  9%|▉         | 35/370 [01:17<12:09,  2.18s/it] 10%|▉         | 36/370 [01:20<12:39,  2.27s/it] 10%|█         | 37/370 [01:22<13:03,  2.35s/it] 10%|█         | 38/370 [01:24<12:24,  2.24s/it] 11%|█         | 39/370 [01:27<12:52,  2.33s/it] 11%|█         | 40/370 [01:29<12:12,  2.22s/it]                                                 11%|█         | 40/370 [01:29<12:12,  2.22s/it] 11%|█         | 41/370 [01:31<11:55,  2.17s/it] 11%|█▏        | 42/370 [01:33<11:39,  2.13s/it] 12%|█▏        | 43/370 [01:35<11:17,  2.07s/it] 12%|█▏        | 44/370 [01:37<11:45,  2.16s/it] 12%|█▏        | 45/370 [01:40<12:06,  2.23s/it] 12%|█▏        | 46/370 [01:42<12:12,  2.26s/it] 13%|█▎        | 47/370 [01:45<12:36,  2.34s/it] 13%|█▎        | 48/370 [01:47<12:12,  2.27s/it] 13%|█▎        | 49/370 [01:49<12:07,  2.27s/it] 14%|█▎        | 50/370 [01:51<11:47,  2.21s/it]                                                 14%|█▎        | 50/370 [01:51<11:47,  2.21s/it]{'loss': 3.5663, 'grad_norm': 3.944272041320801, 'learning_rate': 9.473684210526316e-05, 'epoch': 0.14}
{'loss': 1.3336, 'grad_norm': 1.5795196294784546, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 1.0922, 'grad_norm': 1.23119056224823, 'learning_rate': 0.0001995997184521495, 'epoch': 0.41}
{'loss': 1.106, 'grad_norm': 0.9727154970169067, 'learning_rate': 0.000198402078314949, 'epoch': 0.54}
{'loss': 1.0279, 'grad_norm': 0.9798598289489746, 'learning_rate': 0.00019641666745335624, 'epoch': 0.68}

  0%|          | 0/31 [00:00<?, ?it/s][A
 10%|▉         | 3/31 [00:00<00:01, 27.25it/s][A
 19%|█▉        | 6/31 [00:00<00:01, 17.44it/s][A
 26%|██▌       | 8/31 [00:00<00:01, 14.42it/s][A
 32%|███▏      | 10/31 [00:00<00:01, 13.45it/s][A
 39%|███▊      | 12/31 [00:00<00:01, 13.59it/s][A
 45%|████▌     | 14/31 [00:00<00:01, 12.89it/s][A
 52%|█████▏    | 16/31 [00:01<00:01, 13.43it/s][A
 58%|█████▊    | 18/31 [00:01<00:01, 12.23it/s][A
 65%|██████▍   | 20/31 [00:01<00:00, 12.97it/s][A
 71%|███████   | 22/31 [00:01<00:00, 12.97it/s][A
 77%|███████▋  | 24/31 [00:01<00:00, 12.41it/s][A
 84%|████████▍ | 26/31 [00:01<00:00, 13.45it/s][A
 90%|█████████ | 28/31 [00:02<00:00, 13.90it/s][A
 97%|█████████▋| 30/31 [00:02<00:00, 12.07it/s][ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

                                               [A                                                
100%|██████████| 31/31 [00:05<00:00, 12.07it/s][A 14%|█▎        | 50/370 [01:57<11:47,  2.21s/it]
                                               [A 14%|█▍        | 51/370 [01:59<20:50,  3.92s/it] 14%|█▍        | 52/370 [02:01<18:05,  3.41s/it] 14%|█▍        | 53/370 [02:03<15:33,  2.94s/it] 15%|█▍        | 54/370 [02:05<14:01,  2.66s/it] 15%|█▍        | 55/370 [02:07<13:02,  2.48s/it] 15%|█▌        | 56/370 [02:09<12:30,  2.39s/it] 15%|█▌        | 57/370 [02:11<11:57,  2.29s/it] 16%|█▌        | 58/370 [02:13<11:25,  2.20s/it] 16%|█▌        | 59/370 [02:15<11:11,  2.16s/it] 16%|█▌        | 60/370 [02:18<11:56,  2.31s/it]                                                 16%|█▌        | 60/370 [02:18<11:56,  2.31s/it] 16%|█▋        | 61/370 [02:20<11:29,  2.23s/it] 17%|█▋        | 62/370 [02:22<11:24,  2.22s/it] 17%|█▋        | 63/370 [02:24<11:15,  2.20s/it] 17%|█▋        | 64/370 [02:26<11:01,  2.16s/it] 18%|█▊        | 65/370 [02:29<11:35,  2.28s/it] 18%|█▊        | 66/370 [02:31<11:38,  2.30s/it] 18%|█▊        | 67/370 [02:34<11:49,  2.34s/it] 18%|█▊        | 68/370 [02:36<11:34,  2.30s/it] 19%|█▊        | 69/370 [02:38<11:52,  2.37s/it] 19%|█▉        | 70/370 [02:40<10:57,  2.19s/it]                                                 19%|█▉        | 70/370 [02:40<10:57,  2.19s/it] 19%|█▉        | 71/370 [02:42<10:28,  2.10s/it] 19%|█▉        | 72/370 [02:45<10:54,  2.19s/it] 20%|█▉        | 73/370 [02:46<10:24,  2.10s/it] 20%|██        | 74/370 [02:47<08:41,  1.76s/it] 20%|██        | 75/370 [02:49<09:05,  1.85s/it] 21%|██        | 76/370 [02:52<09:38,  1.97s/it] 21%|██        | 77/370 [02:54<10:09,  2.08s/it] 21%|██        | 78/370 [02:57<10:44,  2.21s/it] 21%|██▏       | 79/370 [02:58<10:08,  2.09s/it] 22%|██▏       | 80/370 [03:00<09:37,  1.99s/it]                                                 22%|██▏       | 80/370 [03:00<09:37,  1.99s/it] 22%|██▏       | 81/370 [03:03<10:07,  2.10s/it] 22%|██▏       | 82/370 [03:05<10:37,  2.21s/it] 22%|██▏       | 83/370 [03:07<10:27,  2.19s/it] 23%|██▎       | 84/370 [03:09<10:04,  2.11s/it] 23%|██▎       | 85/370 [03:11<09:45,  2.05s/it] 23%|██▎       | 86/370 [03:13<09:38,  2.04s/it] 24%|██▎       | 87/370 [03:15<09:42,  2.06s/it] 24%|██▍       | 88/370 [03:17<10:05,  2.15s/it] 24%|██▍       | 89/370 [03:19<09:49,  2.10s/it] 24%|██▍       | 90/370 [03:21<09:28,  2.03s/it]                                                 24%|██▍       | 90/370 [03:21<09:28,  2.03s/it] 25%|██▍       | 91/370 [03:24<10:23,  2.23s/it] 25%|██▍       | 92/370 [03:26<10:23,  2.24s/it] 25%|██▌       | 93/370 [03:29<10:31,  2.28s/it] 25%|██▌       | 94/370 [03:31<11:00,  2.39s/it] 26%|██▌       | 95/370 [03:33<10:15,  2.24s/it] 26%|██▌       | 96/370 [03:35<09:54,  2.17s/it] 26%|██▌       | 97/370 [03:37<09:20,  2.05s/it] 26%|██▋       | 98/370 [03:39<09:49,  2.17s/it] 27%|██▋       | 99/370 [03:41<09:13,  2.04s/it] 27%|██▋       | 100/370 [03:43<09:00,  2.00s/it]                                                  27%|██▋       | 100/370 [03:43<09:00,  2.00s/it]{'eval_loss': 0.8966543078422546, 'eval_rouge1': 0.04114062288532635, 'eval_rouge2': 0.024690475337474115, 'eval_rougeL': 0.03601707169028581, 'eval_bertscore_f1': -0.2912551462650299, 'eval_runtime': 5.8022, 'eval_samples_per_second': 5.343, 'eval_steps_per_second': 5.343, 'epoch': 0.68}
{'loss': 0.982, 'grad_norm': 0.847430944442749, 'learning_rate': 0.00019365938033402715, 'epoch': 0.82}
{'loss': 1.0887, 'grad_norm': 0.9811228513717651, 'learning_rate': 0.00019015229078008165, 'epoch': 0.95}
{'loss': 0.8923, 'grad_norm': 0.8023732900619507, 'learning_rate': 0.0001859234752562217, 'epoch': 1.08}
{'loss': 0.7237, 'grad_norm': 0.7282617688179016, 'learning_rate': 0.00018100678809891666, 'epoch': 1.22}
{'loss': 0.6366, 'grad_norm': 1.273258924484253, 'learning_rate': 0.00017544159049107902, 'epoch': 1.35}

  0%|          | 0/31 [00:00<?, ?it/s][A
 10%|▉         | 3/31 [00:00<00:01, 26.63it/s][A
 19%|█▉        | 6/31 [00:00<00:01, 17.66it/s][A
 26%|██▌       | 8/31 [00:00<00:01, 14.80it/s][A
 32%|███▏      | 10/31 [00:00<00:01, 13.80it/s][A
 39%|███▊      | 12/31 [00:00<00:01, 13.89it/s][A
 45%|████▌     | 14/31 [00:00<00:01, 13.09it/s][A
 52%|█████▏    | 16/31 [00:01<00:01, 13.78it/s][A
 58%|█████▊    | 18/31 [00:01<00:01, 12.58it/s][A
 65%|██████▍   | 20/31 [00:01<00:00, 13.41it/s][A
 71%|███████   | 22/31 [00:01<00:00, 13.32it/s][A
 77%|███████▋  | 24/31 [00:01<00:00, 12.76it/s][A
 84%|████████▍ | 26/31 [00:01<00:00, 13.83it/s][A
 90%|█████████ | 28/31 [00:01<00:00, 14.30it/s][A
 97%|█████████▋| 30/31 [00:02<00:00, 13.16it/s][ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
                                                 
                                               [A 27%|██▋       | 100/370 [03:49<09:00,  2.00s/it]
100%|██████████| 31/31 [00:05<00:00, 13.16it/s][A
                                               [A 27%|██▋       | 101/370 [03:52<18:36,  4.15s/it] 28%|██▊       | 102/370 [03:55<16:03,  3.60s/it] 28%|██▊       | 103/370 [03:57<14:23,  3.23s/it] 28%|██▊       | 104/370 [03:59<13:08,  2.96s/it] 28%|██▊       | 105/370 [04:01<11:53,  2.69s/it] 29%|██▊       | 106/370 [04:03<11:11,  2.55s/it] 29%|██▉       | 107/370 [04:06<11:04,  2.53s/it] 29%|██▉       | 108/370 [04:08<10:58,  2.51s/it] 29%|██▉       | 109/370 [04:11<10:21,  2.38s/it] 30%|██▉       | 110/370 [04:13<09:54,  2.29s/it]                                                  30%|██▉       | 110/370 [04:13<09:54,  2.29s/it] 30%|███       | 111/370 [04:15<10:02,  2.33s/it] 30%|███       | 112/370 [04:17<10:03,  2.34s/it] 31%|███       | 113/370 [04:19<09:42,  2.27s/it] 31%|███       | 114/370 [04:22<09:41,  2.27s/it] 31%|███       | 115/370 [04:24<09:43,  2.29s/it] 31%|███▏      | 116/370 [04:26<09:23,  2.22s/it] 32%|███▏      | 117/370 [04:28<08:58,  2.13s/it] 32%|███▏      | 118/370 [04:30<08:56,  2.13s/it] 32%|███▏      | 119/370 [04:32<08:43,  2.08s/it] 32%|███▏      | 120/370 [04:34<08:57,  2.15s/it]                                                  32%|███▏      | 120/370 [04:34<08:57,  2.15s/it] 33%|███▎      | 121/370 [04:37<09:13,  2.22s/it] 33%|███▎      | 122/370 [04:39<09:05,  2.20s/it] 33%|███▎      | 123/370 [04:41<09:16,  2.25s/it] 34%|███▎      | 124/370 [04:43<08:54,  2.17s/it] 34%|███▍      | 125/370 [04:46<08:49,  2.16s/it] 34%|███▍      | 126/370 [04:48<08:48,  2.16s/it] 34%|███▍      | 127/370 [04:50<09:16,  2.29s/it] 35%|███▍      | 128/370 [04:52<08:42,  2.16s/it] 35%|███▍      | 129/370 [04:54<08:54,  2.22s/it] 35%|███▌      | 130/370 [04:57<09:02,  2.26s/it]                                                  35%|███▌      | 130/370 [04:57<09:02,  2.26s/it] 35%|███▌      | 131/370 [04:59<08:52,  2.23s/it] 36%|███▌      | 132/370 [05:01<08:29,  2.14s/it] 36%|███▌      | 133/370 [05:04<09:11,  2.33s/it] 36%|███▌      | 134/370 [05:06<09:03,  2.30s/it] 36%|███▋      | 135/370 [05:08<08:42,  2.22s/it] 37%|███▋      | 136/370 [05:10<08:35,  2.20s/it] 37%|███▋      | 137/370 [05:12<08:08,  2.09s/it] 37%|███▋      | 138/370 [05:14<08:04,  2.09s/it] 38%|███▊      | 139/370 [05:16<08:20,  2.17s/it] 38%|███▊      | 140/370 [05:18<08:13,  2.15s/it]                                                  38%|███▊      | 140/370 [05:18<08:13,  2.15s/it] 38%|███▊      | 141/370 [05:20<07:50,  2.06s/it] 38%|███▊      | 142/370 [05:22<07:55,  2.08s/it] 39%|███▊      | 143/370 [05:24<07:36,  2.01s/it] 39%|███▉      | 144/370 [05:27<08:10,  2.17s/it] 39%|███▉      | 145/370 [05:29<08:10,  2.18s/it] 39%|███▉      | 146/370 [05:31<08:05,  2.17s/it] 40%|███▉      | 147/370 [05:34<08:19,  2.24s/it] 40%|████      | 148/370 [05:34<06:43,  1.82s/it] 40%|████      | 149/370 [05:37<07:07,  1.93s/it] 41%|████      | 150/370 [05:39<07:01,  1.92s/it]                                                  41%|████      | 150/370 [05:39<07:01,  1.92s/it]{'eval_loss': 1.0153601169586182, 'eval_rouge1': 0.03972562803682119, 'eval_rouge2': 0.022764819747217245, 'eval_rougeL': 0.033774090204123614, 'eval_bertscore_f1': -0.2924993634223938, 'eval_runtime': 5.5024, 'eval_samples_per_second': 5.634, 'eval_steps_per_second': 5.634, 'epoch': 1.35}
{'loss': 0.6495, 'grad_norm': 1.8834902048110962, 'learning_rate': 0.00016927243535095997, 'epoch': 1.49}
{'loss': 0.6369, 'grad_norm': 2.244788885116577, 'learning_rate': 0.00016254871065792776, 'epoch': 1.63}
{'loss': 0.6264, 'grad_norm': 1.6579421758651733, 'learning_rate': 0.0001553242440705314, 'epoch': 1.76}
{'loss': 0.6157, 'grad_norm': 1.8849995136260986, 'learning_rate': 0.0001476568720021308, 'epoch': 1.9}
{'loss': 0.5166, 'grad_norm': 0.9018523693084717, 'learning_rate': 0.0001396079766039157, 'epoch': 2.03}

  0%|          | 0/31 [00:00<?, ?it/s][A
 10%|▉         | 3/31 [00:00<00:01, 26.80it/s][A
 19%|█▉        | 6/31 [00:00<00:01, 17.72it/s][A
 26%|██▌       | 8/31 [00:00<00:01, 14.82it/s][A
 32%|███▏      | 10/31 [00:00<00:01, 13.84it/s][A
 39%|███▊      | 12/31 [00:00<00:01, 13.93it/s][A
 45%|████▌     | 14/31 [00:00<00:01, 13.14it/s][A
 52%|█████▏    | 16/31 [00:01<00:01, 13.76it/s][A
 58%|█████▊    | 18/31 [00:01<00:01, 12.57it/s][A
 65%|██████▍   | 20/31 [00:01<00:00, 13.34it/s][A
 71%|███████   | 22/31 [00:01<00:00, 13.23it/s][A
 77%|███████▋  | 24/31 [00:01<00:00, 12.69it/s][A
 84%|████████▍ | 26/31 [00:01<00:00, 13.80it/s][A
 90%|█████████ | 28/31 [00:01<00:00, 14.29it/s][A
 97%|█████████▋| 30/31 [00:02<00:00, 13.15it/s][ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
                                                 
                                               [A 41%|████      | 150/370 [05:44<07:01,  1.92s/it]
100%|██████████| 31/31 [00:05<00:00, 13.15it/s][A
                                               [A 41%|████      | 151/370 [05:46<13:20,  3.66s/it] 41%|████      | 152/370 [05:48<11:18,  3.11s/it] 41%|████▏     | 153/370 [05:50<09:59,  2.76s/it] 42%|████▏     | 154/370 [05:52<08:55,  2.48s/it] 42%|████▏     | 155/370 [05:54<09:03,  2.53s/it] 42%|████▏     | 156/370 [05:57<08:38,  2.42s/it] 42%|████▏     | 157/370 [05:58<07:57,  2.24s/it] 43%|████▎     | 158/370 [06:01<08:18,  2.35s/it] 43%|████▎     | 159/370 [06:03<07:44,  2.20s/it] 43%|████▎     | 160/370 [06:05<07:27,  2.13s/it]                                                  43%|████▎     | 160/370 [06:05<07:27,  2.13s/it] 44%|████▎     | 161/370 [06:07<07:28,  2.15s/it] 44%|████▍     | 162/370 [06:09<07:03,  2.04s/it] 44%|████▍     | 163/370 [06:11<06:56,  2.01s/it] 44%|████▍     | 164/370 [06:13<07:09,  2.08s/it] 45%|████▍     | 165/370 [06:15<07:21,  2.15s/it] 45%|████▍     | 166/370 [06:17<06:54,  2.03s/it] 45%|████▌     | 167/370 [06:19<06:38,  1.96s/it] 45%|████▌     | 168/370 [06:22<07:26,  2.21s/it] 46%|████▌     | 169/370 [06:24<07:13,  2.16s/it] 46%|████▌     | 170/370 [06:26<06:58,  2.09s/it]                                                  46%|████▌     | 170/370 [06:26<06:58,  2.09s/it] 46%|████▌     | 171/370 [06:28<07:25,  2.24s/it] 46%|████▋     | 172/370 [06:31<07:41,  2.33s/it] 47%|████▋     | 173/370 [06:33<07:44,  2.36s/it] 47%|████▋     | 174/370 [06:35<07:17,  2.23s/it] 47%|████▋     | 175/370 [06:37<07:08,  2.19s/it] 48%|████▊     | 176/370 [06:40<07:08,  2.21s/it] 48%|████▊     | 177/370 [06:42<06:56,  2.16s/it] 48%|████▊     | 178/370 [06:44<06:49,  2.13s/it] 48%|████▊     | 179/370 [06:46<06:48,  2.14s/it] 49%|████▊     | 180/370 [06:48<07:14,  2.29s/it]                                                  49%|████▊     | 180/370 [06:48<07:14,  2.29s/it] 49%|████▉     | 181/370 [06:51<06:59,  2.22s/it] 49%|████▉     | 182/370 [06:53<07:19,  2.34s/it] 49%|████▉     | 183/370 [06:55<06:57,  2.23s/it] 50%|████▉     | 184/370 [06:57<06:31,  2.11s/it] 50%|█████     | 185/370 [07:00<07:05,  2.30s/it] 50%|█████     | 186/370 [07:02<06:39,  2.17s/it] 51%|█████     | 187/370 [07:03<06:10,  2.02s/it] 51%|█████     | 188/370 [07:05<06:07,  2.02s/it] 51%|█████     | 189/370 [07:07<06:17,  2.08s/it] 51%|█████▏    | 190/370 [07:10<06:15,  2.09s/it]                                                  51%|█████▏    | 190/370 [07:10<06:15,  2.09s/it] 52%|█████▏    | 191/370 [07:11<06:02,  2.02s/it] 52%|█████▏    | 192/370 [07:14<06:08,  2.07s/it] 52%|█████▏    | 193/370 [07:16<06:15,  2.12s/it] 52%|█████▏    | 194/370 [07:18<06:26,  2.19s/it] 53%|█████▎    | 195/370 [07:21<06:51,  2.35s/it] 53%|█████▎    | 196/370 [07:23<06:25,  2.22s/it] 53%|█████▎    | 197/370 [07:25<06:46,  2.35s/it] 54%|█████▎    | 198/370 [07:28<06:54,  2.41s/it] 54%|█████▍    | 199/370 [07:30<06:39,  2.33s/it] 54%|█████▍    | 200/370 [07:33<06:55,  2.44s/it]                                                  54%|█████▍    | 200/370 [07:33<06:55,  2.44s/it]{'eval_loss': 1.0313082933425903, 'eval_rouge1': 0.04072781660500158, 'eval_rouge2': 0.022539698860599332, 'eval_rougeL': 0.03407638796294646, 'eval_bertscore_f1': -0.28435641527175903, 'eval_runtime': 5.4781, 'eval_samples_per_second': 5.659, 'eval_steps_per_second': 5.659, 'epoch': 2.03}
{'loss': 0.1614, 'grad_norm': 1.9881783723831177, 'learning_rate': 0.00013124199436205576, 'epoch': 2.16}
{'loss': 0.1615, 'grad_norm': 1.2229747772216797, 'learning_rate': 0.00012262590024297225, 'epoch': 2.3}
{'loss': 0.1389, 'grad_norm': 1.1344534158706665, 'learning_rate': 0.00011382867151647332, 'epoch': 2.44}
{'loss': 0.1189, 'grad_norm': 1.8040878772735596, 'learning_rate': 0.00010492073554918782, 'epoch': 2.57}
{'loss': 0.1824, 'grad_norm': 2.237020492553711, 'learning_rate': 9.597340598905852e-05, 'epoch': 2.71}

  0%|          | 0/31 [00:00<?, ?it/s][A
 10%|▉         | 3/31 [00:00<00:01, 26.59it/s][A
 19%|█▉        | 6/31 [00:00<00:01, 17.69it/s][A
 26%|██▌       | 8/31 [00:00<00:01, 14.79it/s][A
 32%|███▏      | 10/31 [00:00<00:01, 13.78it/s][A
 39%|███▊      | 12/31 [00:00<00:01, 13.90it/s][A
 45%|████▌     | 14/31 [00:00<00:01, 13.12it/s][A
 52%|█████▏    | 16/31 [00:01<00:01, 13.79it/s][A
 58%|█████▊    | 18/31 [00:01<00:01, 12.58it/s][A
 65%|██████▍   | 20/31 [00:01<00:00, 13.37it/s][A
 71%|███████   | 22/31 [00:01<00:00, 13.27it/s][A
 77%|███████▋  | 24/31 [00:01<00:00, 12.70it/s][A
 84%|████████▍ | 26/31 [00:01<00:00, 13.80it/s][A
 90%|█████████ | 28/31 [00:01<00:00, 14.31it/s][A
 97%|█████████▋| 30/31 [00:02<00:00, 13.12it/s][ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
                                                 
                                               [A 54%|█████▍    | 200/370 [07:39<06:55,  2.44s/it]
100%|██████████| 31/31 [00:05<00:00, 13.12it/s][A
                                               [A 54%|█████▍    | 201/370 [07:42<12:26,  4.42s/it] 55%|█████▍    | 202/370 [07:44<10:39,  3.81s/it] 55%|█████▍    | 203/370 [07:46<09:10,  3.30s/it] 55%|█████▌    | 204/370 [07:48<08:02,  2.91s/it] 55%|█████▌    | 205/370 [07:50<07:10,  2.61s/it] 56%|█████▌    | 206/370 [07:52<06:36,  2.41s/it] 56%|█████▌    | 207/370 [07:54<06:18,  2.32s/it] 56%|█████▌    | 208/370 [07:57<06:42,  2.49s/it] 56%|█████▋    | 209/370 [08:00<06:35,  2.45s/it] 57%|█████▋    | 210/370 [08:02<06:05,  2.29s/it]                                                  57%|█████▋    | 210/370 [08:02<06:05,  2.29s/it] 57%|█████▋    | 211/370 [08:03<05:45,  2.17s/it] 57%|█████▋    | 212/370 [08:06<06:22,  2.42s/it] 58%|█████▊    | 213/370 [08:08<06:01,  2.30s/it] 58%|█████▊    | 214/370 [08:11<06:14,  2.40s/it] 58%|█████▊    | 215/370 [08:13<05:44,  2.22s/it] 58%|█████▊    | 216/370 [08:15<05:26,  2.12s/it] 59%|█████▊    | 217/370 [08:17<05:25,  2.13s/it] 59%|█████▉    | 218/370 [08:19<05:17,  2.09s/it] 59%|█████▉    | 219/370 [08:21<05:17,  2.10s/it] 59%|█████▉    | 220/370 [08:23<05:15,  2.10s/it]                                                  59%|█████▉    | 220/370 [08:23<05:15,  2.10s/it] 60%|█████▉    | 221/370 [08:26<05:25,  2.18s/it] 60%|██████    | 222/370 [08:27<04:32,  1.84s/it] 60%|██████    | 223/370 [08:28<04:26,  1.81s/it] 61%|██████    | 224/370 [08:31<04:43,  1.94s/it] 61%|██████    | 225/370 [08:33<04:48,  1.99s/it] 61%|██████    | 226/370 [08:35<04:52,  2.03s/it] 61%|██████▏   | 227/370 [08:37<05:05,  2.13s/it] 62%|██████▏   | 228/370 [08:39<04:50,  2.05s/it] 62%|██████▏   | 229/370 [08:41<04:59,  2.13s/it] 62%|██████▏   | 230/370 [08:44<05:28,  2.34s/it]                                                  62%|██████▏   | 230/370 [08:44<05:28,  2.34s/it] 62%|██████▏   | 231/370 [08:46<05:15,  2.27s/it] 63%|██████▎   | 232/370 [08:49<05:23,  2.34s/it] 63%|██████▎   | 233/370 [08:51<05:26,  2.38s/it] 63%|██████▎   | 234/370 [08:53<05:11,  2.29s/it] 64%|██████▎   | 235/370 [08:55<05:01,  2.23s/it] 64%|██████▍   | 236/370 [08:57<04:51,  2.17s/it] 64%|██████▍   | 237/370 [09:00<05:05,  2.30s/it] 64%|██████▍   | 238/370 [09:02<04:50,  2.20s/it] 65%|██████▍   | 239/370 [09:04<04:42,  2.15s/it] 65%|██████▍   | 240/370 [09:06<04:41,  2.16s/it]                                                  65%|██████▍   | 240/370 [09:06<04:41,  2.16s/it] 65%|██████▌   | 241/370 [09:08<04:32,  2.11s/it] 65%|██████▌   | 242/370 [09:10<04:32,  2.13s/it] 66%|██████▌   | 243/370 [09:13<04:32,  2.14s/it] 66%|██████▌   | 244/370 [09:15<04:23,  2.09s/it] 66%|██████▌   | 245/370 [09:17<04:14,  2.04s/it] 66%|██████▋   | 246/370 [09:18<04:02,  1.96s/it] 67%|██████▋   | 247/370 [09:20<04:01,  1.97s/it] 67%|██████▋   | 248/370 [09:23<04:13,  2.08s/it] 67%|██████▋   | 249/370 [09:25<04:08,  2.05s/it] 68%|██████▊   | 250/370 [09:27<04:17,  2.14s/it]                                                  68%|██████▊   | 250/370 [09:27<04:17,  2.14s/it]{'eval_loss': 1.3578029870986938, 'eval_rouge1': 0.040007953030913236, 'eval_rouge2': 0.02252757874528657, 'eval_rougeL': 0.03383274699396342, 'eval_bertscore_f1': -0.2882690131664276, 'eval_runtime': 5.6646, 'eval_samples_per_second': 5.473, 'eval_steps_per_second': 5.473, 'epoch': 2.71}
{'loss': 0.1273, 'grad_norm': 1.2213712930679321, 'learning_rate': 8.705831185459445e-05, 'epoch': 2.84}
{'loss': 0.1511, 'grad_norm': 0.8941236734390259, 'learning_rate': 7.824682409938328e-05, 'epoch': 2.98}
{'loss': 0.0562, 'grad_norm': 0.5197882056236267, 'learning_rate': 6.960948424257532e-05, 'epoch': 3.11}
{'loss': 0.0436, 'grad_norm': 0.8513403534889221, 'learning_rate': 6.121543963951452e-05, 'epoch': 3.24}
{'loss': 0.0383, 'grad_norm': 1.3922744989395142, 'learning_rate': 5.313188991352964e-05, 'epoch': 3.38}

  0%|          | 0/31 [00:00<?, ?it/s][A
 10%|▉         | 3/31 [00:00<00:01, 27.17it/s][A
 19%|█▉        | 6/31 [00:00<00:01, 17.72it/s][A
 26%|██▌       | 8/31 [00:00<00:01, 14.81it/s][A
 32%|███▏      | 10/31 [00:00<00:01, 13.85it/s][A
 39%|███▊      | 12/31 [00:00<00:01, 13.98it/s][A
 45%|████▌     | 14/31 [00:00<00:01, 13.14it/s][A
 52%|█████▏    | 16/31 [00:01<00:01, 13.77it/s][A
 58%|█████▊    | 18/31 [00:01<00:01, 12.55it/s][A
 65%|██████▍   | 20/31 [00:01<00:00, 13.35it/s][A
 71%|███████   | 22/31 [00:01<00:00, 13.27it/s][A
 77%|███████▋  | 24/31 [00:01<00:00, 12.71it/s][A
 84%|████████▍ | 26/31 [00:01<00:00, 13.77it/s][A
 90%|█████████ | 28/31 [00:01<00:00, 14.27it/s][A
 97%|█████████▋| 30/31 [00:02<00:00, 13.12it/s][ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
                                                 
                                               [A 68%|██████▊   | 250/370 [09:32<04:17,  2.14s/it]
100%|██████████| 31/31 [00:05<00:00, 13.12it/s][A
                                               [A 68%|██████▊   | 251/370 [09:35<07:32,  3.80s/it] 68%|██████▊   | 252/370 [09:37<06:27,  3.29s/it] 68%|██████▊   | 253/370 [09:39<05:37,  2.88s/it] 69%|██████▊   | 254/370 [09:41<05:12,  2.70s/it] 69%|██████▉   | 255/370 [09:43<04:56,  2.58s/it] 69%|██████▉   | 256/370 [09:46<04:47,  2.52s/it] 69%|██████▉   | 257/370 [09:48<04:40,  2.49s/it] 70%|██████▉   | 258/370 [09:50<04:23,  2.36s/it] 70%|███████   | 259/370 [09:52<04:23,  2.37s/it] 70%|███████   | 260/370 [09:54<04:04,  2.22s/it]                                                  70%|███████   | 260/370 [09:54<04:04,  2.22s/it] 71%|███████   | 261/370 [09:57<04:03,  2.24s/it] 71%|███████   | 262/370 [09:58<03:49,  2.13s/it] 71%|███████   | 263/370 [10:01<03:50,  2.15s/it] 71%|███████▏  | 264/370 [10:03<04:02,  2.29s/it] 72%|███████▏  | 265/370 [10:05<03:50,  2.19s/it] 72%|███████▏  | 266/370 [10:08<04:05,  2.36s/it] 72%|███████▏  | 267/370 [10:10<03:53,  2.26s/it] 72%|███████▏  | 268/370 [10:13<04:17,  2.53s/it] 73%|███████▎  | 269/370 [10:15<04:02,  2.40s/it] 73%|███████▎  | 270/370 [10:17<03:50,  2.31s/it]                                                  73%|███████▎  | 270/370 [10:17<03:50,  2.31s/it] 73%|███████▎  | 271/370 [10:20<03:45,  2.28s/it] 74%|███████▎  | 272/370 [10:22<03:38,  2.23s/it] 74%|███████▍  | 273/370 [10:24<03:40,  2.27s/it] 74%|███████▍  | 274/370 [10:26<03:30,  2.19s/it] 74%|███████▍  | 275/370 [10:28<03:32,  2.24s/it] 75%|███████▍  | 276/370 [10:30<03:23,  2.17s/it] 75%|███████▍  | 277/370 [10:33<03:31,  2.28s/it] 75%|███████▌  | 278/370 [10:35<03:22,  2.20s/it] 75%|███████▌  | 279/370 [10:37<03:07,  2.06s/it] 76%|███████▌  | 280/370 [10:39<03:08,  2.10s/it]                                                  76%|███████▌  | 280/370 [10:39<03:08,  2.10s/it] 76%|███████▌  | 281/370 [10:42<03:20,  2.25s/it] 76%|███████▌  | 282/370 [10:44<03:27,  2.35s/it] 76%|███████▋  | 283/370 [10:46<03:11,  2.20s/it] 77%|███████▋  | 284/370 [10:48<03:04,  2.15s/it] 77%|███████▋  | 285/370 [10:50<02:55,  2.07s/it] 77%|███████▋  | 286/370 [10:52<02:56,  2.11s/it] 78%|███████▊  | 287/370 [10:54<02:53,  2.09s/it] 78%|███████▊  | 288/370 [10:56<02:44,  2.00s/it] 78%|███████▊  | 289/370 [10:58<02:52,  2.13s/it] 78%|███████▊  | 290/370 [11:00<02:49,  2.12s/it]                                                  78%|███████▊  | 290/370 [11:00<02:49,  2.12s/it] 79%|███████▊  | 291/370 [11:03<02:48,  2.13s/it] 79%|███████▉  | 292/370 [11:05<02:48,  2.16s/it] 79%|███████▉  | 293/370 [11:07<02:44,  2.14s/it] 79%|███████▉  | 294/370 [11:09<02:36,  2.06s/it] 80%|███████▉  | 295/370 [11:11<02:31,  2.01s/it] 80%|████████  | 296/370 [11:12<02:07,  1.72s/it] 80%|████████  | 297/370 [11:14<02:22,  1.95s/it] 81%|████████  | 298/370 [11:16<02:23,  2.00s/it] 81%|████████  | 299/370 [11:18<02:16,  1.92s/it] 81%|████████  | 300/370 [11:20<02:15,  1.93s/it]                                                  81%|████████  | 300/370 [11:20<02:15,  1.93s/it]{'eval_loss': 1.7671397924423218, 'eval_rouge1': 0.039894311343912164, 'eval_rouge2': 0.022094527858676542, 'eval_rougeL': 0.03385056795436189, 'eval_bertscore_f1': -0.2969897389411926, 'eval_runtime': 5.5037, 'eval_samples_per_second': 5.633, 'eval_steps_per_second': 5.633, 'epoch': 3.38}
{'loss': 0.0519, 'grad_norm': 1.0554957389831543, 'learning_rate': 4.542354898054953e-05, 'epoch': 3.52}
{'loss': 0.0563, 'grad_norm': 0.6462181806564331, 'learning_rate': 3.815212697337451e-05, 'epoch': 3.65}
{'loss': 0.0435, 'grad_norm': 1.3240197896957397, 'learning_rate': 3.137583621312665e-05, 'epoch': 3.79}
{'loss': 0.0285, 'grad_norm': 1.7712106704711914, 'learning_rate': 2.514892518288988e-05, 'epoch': 3.93}
{'loss': 0.0472, 'grad_norm': 0.3581281900405884, 'learning_rate': 1.952124423437447e-05, 'epoch': 4.05}

  0%|          | 0/31 [00:00<?, ?it/s][A
 10%|▉         | 3/31 [00:00<00:01, 26.87it/s][A
 19%|█▉        | 6/31 [00:00<00:01, 17.78it/s][A
 26%|██▌       | 8/31 [00:00<00:01, 14.85it/s][A
 32%|███▏      | 10/31 [00:00<00:01, 13.86it/s][A
 39%|███▊      | 12/31 [00:00<00:01, 13.91it/s][A
 45%|████▌     | 14/31 [00:00<00:01, 13.12it/s][A
 52%|█████▏    | 16/31 [00:01<00:01, 13.78it/s][A
 58%|█████▊    | 18/31 [00:01<00:01, 12.59it/s][A
 65%|██████▍   | 20/31 [00:01<00:00, 13.40it/s][A
 71%|███████   | 22/31 [00:01<00:00, 13.29it/s][A
 77%|███████▋  | 24/31 [00:01<00:00, 12.71it/s][A
 84%|████████▍ | 26/31 [00:01<00:00, 13.79it/s][A
 90%|█████████ | 28/31 [00:01<00:00, 14.28it/s][A
 97%|█████████▋| 30/31 [00:02<00:00, 13.14it/s][ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
                                                 
                                               [A 81%|████████  | 300/370 [11:25<02:15,  1.93s/it]
100%|██████████| 31/31 [00:05<00:00, 13.14it/s][A
                                               [A 81%|████████▏ | 301/370 [11:29<04:33,  3.96s/it] 82%|████████▏ | 302/370 [11:31<03:55,  3.47s/it] 82%|████████▏ | 303/370 [11:33<03:25,  3.06s/it] 82%|████████▏ | 304/370 [11:35<03:02,  2.76s/it] 82%|████████▏ | 305/370 [11:38<03:07,  2.88s/it] 83%|████████▎ | 306/370 [11:41<03:05,  2.89s/it] 83%|████████▎ | 307/370 [11:44<02:52,  2.73s/it] 83%|████████▎ | 308/370 [11:46<02:40,  2.58s/it] 84%|████████▎ | 309/370 [11:48<02:36,  2.57s/it] 84%|████████▍ | 310/370 [11:50<02:22,  2.38s/it]                                                  84%|████████▍ | 310/370 [11:50<02:22,  2.38s/it] 84%|████████▍ | 311/370 [11:53<02:19,  2.37s/it] 84%|████████▍ | 312/370 [11:55<02:16,  2.35s/it] 85%|████████▍ | 313/370 [11:58<02:20,  2.47s/it] 85%|████████▍ | 314/370 [12:00<02:08,  2.30s/it] 85%|████████▌ | 315/370 [12:02<02:01,  2.21s/it] 85%|████████▌ | 316/370 [12:04<01:57,  2.18s/it] 86%|████████▌ | 317/370 [12:06<01:54,  2.16s/it] 86%|████████▌ | 318/370 [12:08<01:52,  2.17s/it] 86%|████████▌ | 319/370 [12:10<01:48,  2.13s/it] 86%|████████▋ | 320/370 [12:12<01:48,  2.16s/it]                                                  86%|████████▋ | 320/370 [12:12<01:48,  2.16s/it] 87%|████████▋ | 321/370 [12:15<01:55,  2.36s/it] 87%|████████▋ | 322/370 [12:17<01:48,  2.27s/it] 87%|████████▋ | 323/370 [12:19<01:45,  2.24s/it] 88%|████████▊ | 324/370 [12:22<01:44,  2.28s/it] 88%|████████▊ | 325/370 [12:24<01:41,  2.26s/it] 88%|████████▊ | 326/370 [12:26<01:42,  2.32s/it] 88%|████████▊ | 327/370 [12:28<01:33,  2.18s/it] 89%|████████▊ | 328/370 [12:30<01:30,  2.16s/it] 89%|████████▉ | 329/370 [12:32<01:26,  2.11s/it] 89%|████████▉ | 330/370 [12:34<01:19,  1.98s/it]                                                  89%|████████▉ | 330/370 [12:34<01:19,  1.98s/it] 89%|████████▉ | 331/370 [12:36<01:15,  1.95s/it] 90%|████████▉ | 332/370 [12:38<01:16,  2.00s/it] 90%|█████████ | 333/370 [12:41<01:19,  2.15s/it] 90%|█████████ | 334/370 [12:44<01:26,  2.39s/it] 91%|█████████ | 335/370 [12:46<01:26,  2.48s/it] 91%|█████████ | 336/370 [12:48<01:20,  2.36s/it] 91%|█████████ | 337/370 [12:50<01:16,  2.32s/it] 91%|█████████▏| 338/370 [12:52<01:10,  2.20s/it] 92%|█████████▏| 339/370 [12:54<01:04,  2.07s/it] 92%|█████████▏| 340/370 [12:57<01:04,  2.16s/it]                                                  92%|█████████▏| 340/370 [12:57<01:04,  2.16s/it] 92%|█████████▏| 341/370 [12:59<01:03,  2.21s/it] 92%|█████████▏| 342/370 [13:01<01:02,  2.25s/it] 93%|█████████▎| 343/370 [13:03<00:58,  2.16s/it] 93%|█████████▎| 344/370 [13:05<00:53,  2.07s/it] 93%|█████████▎| 345/370 [13:07<00:51,  2.08s/it] 94%|█████████▎| 346/370 [13:09<00:51,  2.14s/it] 94%|█████████▍| 347/370 [13:12<00:50,  2.20s/it] 94%|█████████▍| 348/370 [13:14<00:48,  2.20s/it] 94%|█████████▍| 349/370 [13:16<00:45,  2.17s/it] 95%|█████████▍| 350/370 [13:18<00:41,  2.09s/it]                                                  95%|█████████▍| 350/370 [13:18<00:41,  2.09s/it]{'eval_loss': 1.6485021114349365, 'eval_rouge1': 0.04032137405567937, 'eval_rouge2': 0.022232778807454626, 'eval_rougeL': 0.03405694218044383, 'eval_bertscore_f1': -0.2879164218902588, 'eval_runtime': 5.4405, 'eval_samples_per_second': 5.698, 'eval_steps_per_second': 5.698, 'epoch': 4.05}
{'loss': 0.0136, 'grad_norm': 0.25386327505111694, 'learning_rate': 1.4537846504397979e-05, 'epoch': 4.19}
{'loss': 0.0135, 'grad_norm': 0.5859295129776001, 'learning_rate': 1.0238627236098619e-05, 'epoch': 4.33}
{'loss': 0.0117, 'grad_norm': 0.33896398544311523, 'learning_rate': 6.658004392341632e-06, 'epoch': 4.46}
{'loss': 0.008, 'grad_norm': 0.33988165855407715, 'learning_rate': 3.824643118210403e-06, 'epoch': 4.6}
{'loss': 0.0071, 'grad_norm': 0.1875421553850174, 'learning_rate': 1.7612262584335237e-06, 'epoch': 4.73}

  0%|          | 0/31 [00:00<?, ?it/s][A
 10%|▉         | 3/31 [00:00<00:01, 26.64it/s][A
 19%|█▉        | 6/31 [00:00<00:01, 17.65it/s][A
 26%|██▌       | 8/31 [00:00<00:01, 14.79it/s][A
 32%|███▏      | 10/31 [00:00<00:01, 13.81it/s][A
 39%|███▊      | 12/31 [00:00<00:01, 13.91it/s][A
 45%|████▌     | 14/31 [00:00<00:01, 13.12it/s][A
 52%|█████▏    | 16/31 [00:01<00:01, 13.77it/s][A
 58%|█████▊    | 18/31 [00:01<00:01, 12.57it/s][A
 65%|██████▍   | 20/31 [00:01<00:00, 13.37it/s][A
 71%|███████   | 22/31 [00:01<00:00, 13.29it/s][A
 77%|███████▋  | 24/31 [00:01<00:00, 12.74it/s][A
 84%|████████▍ | 26/31 [00:01<00:00, 13.81it/s][A
 90%|█████████ | 28/31 [00:01<00:00, 14.32it/s][A
 97%|█████████▋| 30/31 [00:02<00:00, 13.18it/s][ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
                                                 
                                               [A 95%|█████████▍| 350/370 [13:23<00:41,  2.09s/it]
100%|██████████| 31/31 [00:05<00:00, 13.18it/s][A
                                               [A 95%|█████████▍| 351/370 [13:26<01:10,  3.72s/it] 95%|█████████▌| 352/370 [13:28<01:00,  3.35s/it] 95%|█████████▌| 353/370 [13:30<00:50,  3.00s/it] 96%|█████████▌| 354/370 [13:32<00:42,  2.66s/it] 96%|█████████▌| 355/370 [13:34<00:35,  2.38s/it] 96%|█████████▌| 356/370 [13:36<00:33,  2.38s/it] 96%|█████████▋| 357/370 [13:38<00:28,  2.22s/it] 97%|█████████▋| 358/370 [13:40<00:25,  2.09s/it] 97%|█████████▋| 359/370 [13:42<00:22,  2.06s/it] 97%|█████████▋| 360/370 [13:44<00:19,  1.99s/it]                                                  97%|█████████▋| 360/370 [13:44<00:19,  1.99s/it] 98%|█████████▊| 361/370 [13:46<00:18,  2.01s/it] 98%|█████████▊| 362/370 [13:48<00:15,  2.00s/it] 98%|█████████▊| 363/370 [13:49<00:13,  1.93s/it] 98%|█████████▊| 364/370 [13:51<00:11,  1.86s/it] 99%|█████████▊| 365/370 [13:53<00:09,  1.88s/it] 99%|█████████▉| 366/370 [13:55<00:08,  2.01s/it] 99%|█████████▉| 367/370 [13:57<00:06,  2.00s/it] 99%|█████████▉| 368/370 [14:00<00:04,  2.13s/it]100%|█████████▉| 369/370 [14:02<00:02,  2.20s/it]100%|██████████| 370/370 [14:03<00:00,  1.89s/it]                                                 100%|██████████| 370/370 [14:03<00:00,  1.89s/it]                                                 100%|██████████| 370/370 [14:04<00:00,  1.89s/it]100%|██████████| 370/370 [14:04<00:00,  2.28s/it]
{'eval_loss': 1.7433056831359863, 'eval_rouge1': 0.040391102977639516, 'eval_rouge2': 0.02171484140968582, 'eval_rougeL': 0.0337927183809589, 'eval_bertscore_f1': -0.29194819927215576, 'eval_runtime': 5.5025, 'eval_samples_per_second': 5.634, 'eval_steps_per_second': 5.634, 'epoch': 4.73}
{'loss': 0.0059, 'grad_norm': 0.28839194774627686, 'learning_rate': 4.842727669008174e-07, 'epoch': 4.87}
{'loss': 0.0151, 'grad_norm': 1.3267931938171387, 'learning_rate': 4.0054620147667036e-09, 'epoch': 5.0}
{'train_runtime': 844.5551, 'train_samples_per_second': 3.481, 'train_steps_per_second': 0.438, 'train_loss': 0.45881522975459293, 'epoch': 5.0}
Running final evaluation...
  0%|          | 0/31 [00:00<?, ?it/s] 10%|▉         | 3/31 [00:00<00:01, 26.87it/s] 19%|█▉        | 6/31 [00:00<00:01, 17.81it/s] 26%|██▌       | 8/31 [00:00<00:01, 14.91it/s] 32%|███▏      | 10/31 [00:00<00:01, 13.88it/s] 39%|███▊      | 12/31 [00:00<00:01, 13.96it/s] 45%|████▌     | 14/31 [00:00<00:01, 13.14it/s] 52%|█████▏    | 16/31 [00:01<00:01, 13.81it/s] 58%|█████▊    | 18/31 [00:01<00:01, 12.62it/s] 65%|██████▍   | 20/31 [00:01<00:00, 13.42it/s] 71%|███████   | 22/31 [00:01<00:00, 13.34it/s] 77%|███████▋  | 24/31 [00:01<00:00, 12.78it/s] 84%|████████▍ | 26/31 [00:01<00:00, 13.85it/s] 90%|█████████ | 28/31 [00:01<00:00, 14.32it/s] 97%|█████████▋| 30/31 [00:02<00:00, 13.16it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100%|██████████| 31/31 [00:05<00:00,  5.81it/s]
Final evaluation metrics: {'eval_loss': 1.7476117610931396, 'eval_rouge1': 0.040306560695179926, 'eval_rouge2': 0.021879364512295465, 'eval_rougeL': 0.034041093932495936, 'eval_bertscore_f1': -0.29067400097846985, 'eval_runtime': 5.4215, 'eval_samples_per_second': 5.718, 'eval_steps_per_second': 5.718, 'epoch': 5.0}
Saved final metrics to outputs/social_only_Llama-2-13b-chat-hf/final_metrics.csv
