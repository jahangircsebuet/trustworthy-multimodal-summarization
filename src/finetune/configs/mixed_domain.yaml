base_model: "meta-llama/Meta-Llama-3-8B-Instruct"
output_dir: "outputs/mixed_domain"
seed: 42

datasets:
  reddit_tifu: { use: true, subset: "short", split: "train" }
  tweetsumm:   { use: false, path: "data/tweetsumm.jsonl" }
  cnn_dailymail: { use: true, version: "3.0.0", split: "train" }
  multi_news:    { use: true, split: "train" }

max_source_len: 3072
max_target_len: 128
train_ratio: 0.97
val_ratio: 0.03

sampling:
  use_domain_tags: true
  social_to_news_ratio: [0.7, 0.3]   # 70% social, 30% news
  upsample_social: 3                 # optional extra upsampling on social sources

bf16: true
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 16
num_train_epochs: 2
learning_rate: 1.5e-4
weight_decay: 0.01
warmup_ratio: 0.05
lr_scheduler_type: "cosine"
save_steps: 1000
eval_steps: 1000
logging_steps: 50
gradient_checkpointing: true
max_steps: -1

lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"
