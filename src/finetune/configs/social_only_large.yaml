#base_model: "meta-llama/Meta-Llama-3-8B-Instruct"  # meta-llama/Llama-2-13b-chat-hf  # meta-llama/Llama-2-7b-chat-hf   #mistralai/Mixtral-8x7B-v0.1 #mistralai/Mistral-7B-Instruct-v0.2
base_model: "mistralai/Ministral-8B-Instruct-2410"
output_dir: "outputs/Ministral-8B-Instruct-2410"
seed: 42

# data - Use larger dataset
datasets:
  reddit_tifu: { use: true, subset: "long", split: "train" }  # Use "long" instead of "short"
  tweetsumm:   { use: false, path: "data/tweetsumm.jsonl" }

max_source_len: 2048
max_target_len: 96
train_ratio: 0.95  # Use more data for training
val_ratio: 0.05    # Use more data for validation

# sampling / curriculum
sampling:
  use_domain_tags: true
  social_to_news_ratio: [1.0, 0.0]

# training - More frequent logging and evaluation
bf16: true
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8  # Reduced to get more steps
num_train_epochs: 5  # More epochs
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_ratio: 0.05
lr_scheduler_type: "cosine"
save_steps: 100
eval_steps: 50      # Evaluate every 50 steps
logging_steps: 10   # Log every 10 steps
gradient_checkpointing: true
max_steps: -1

# lora
lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"
